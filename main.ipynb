{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "integrantes: \n",
    "\n",
    "- Francis Aguilar #22243\n",
    "\n",
    "- Gerardo Pineda #22808\n",
    "\n",
    "- Angela Garcia #22869\n",
    "\n",
    "enlace al repositorio:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Teoría\n",
    "\n",
    "1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje \n",
    "supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de \n",
    "aprendizaje por refuerzo \n",
    "\n",
    "    El temporal difference learning es un método que combina las ideas del aprendizaje dinámico y del aprendizaje supervisado.\n",
    "    Este tipo de aprendizaje esta basado en aprender a estimar valores futuros miestras va interactuando con un entorno, en vez de\n",
    "    esperar hasta el final de una secuencia para ajustar las predicciones. Lo que hace es actualizar gradualmente las estimaciones en \n",
    "    función de las diferencias entre predicciones consecutivas. Entonces, las diferencias se usan para ajustar el modelo en tiempo real.\n",
    "    Acerca del concepto de error de diferencia temporal, esta en los algoritmos de aprendizaje por refuerzo, porque lo que mide es la discrepancia entre la recompensa esperada y la obtenida. \n",
    "\n",
    "\n",
    "2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones \n",
    "de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego \n",
    "simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación \n",
    "\n",
    "    En los juegos simultáneos un jugador al no conocer las acciones del oponente, busca estrategias considerando las posibles acciones del oponente, \n",
    "    pensando en obtener la mejor recompensa posible y muchas veces se toma en cuenta que al realizar una accion no dar pistas al oponente.\n",
    "    Los jugadores también pueden tomar decisiones en función de la información disponible, como por ejemplo, el estado del juego, \n",
    "    pero también pueden tomar decisiones basadas en su conocimiento del entorno y de la estrategia de sus oponentes.\n",
    "    Un ejemplo del mundo real es el juego de werewolf en el que los jugadores no saben el rol de los demás jugadores, y deben tomar decisiones\n",
    "    a base sin saber si son humanos o lobos, entonces tratan de tomar acciones que no demuestren que rol tienen y simular que todos son humanos, \n",
    "    las acciones en este caso son en las votaciones, porque al decir que alguien es el lobo tiene que tener sentido, ya que al no tenerlo lo pueden \n",
    "    juzgar que es el lobo y terminar muerto dentro del juego. Mientras que los lobos, deben de ser cautelosos al matar a alguien, ya que si se descubren los acusaran de ser el lobo y terminarán muertos.\n",
    "\n",
    "\n",
    "3. ¿Qué distingue los juegos de suma cero de los juegos de suma no cero y cómo afecta esta diferencia al \n",
    "proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren \n",
    "en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas \n",
    "\n",
    "    Lo que distingue a los juegos de suma cero de los juegos de suma de cero son en que los juegos de suma cero los \n",
    "    jugadores suelen actuar de una manera más competitiva porque no hay beneficios que se puedan compartir.\n",
    "    Mientras, en los juegos de suma no cero los jugadores pueden buscar estrategias colaborativas porque hay beneficios que se pueden compartir.\n",
    "\n",
    "    Un ejemplo de los juegos de suma no cero es Genshin impact, porque en este se pueden hacer partidas colaborativas en donde \n",
    "    ambos jugadores obtengan beneficios, como por ejemplo, obtener recompensas, subir de nivel y objeto. Entonces, al entrar a alguna\n",
    "    mundo de otro jugador o cuando entran al mundo de uno, los jugadores pueden buscar una estrategia que maximice sus beneficios y minimice los beneficios de los demás. Incluso otro ejemplo es minecraft en donde muchas veces se juega multijugador mientras uno cumple el rol de construir la casa, otro exploración y otro minar, para obtener mayores recursos y compartirlos.\n",
    "\n",
    "\n",
    "\n",
    "4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de \n",
    "Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse \n",
    "unilateralmente de la estrategia elegida \n",
    "\n",
    "    El concepto de equilibrio de Nash se aplica en los juegos simultáneos porque los jugadores toman decisiones al mismo tiempo sin conocer las elecciones de los demás. Las aplicaciones están en la elección de estrategias ótimas cuando el jugador escoge su mejor respuesta para maximizar su utilidad a base de lo que los demás están haciendo. También, se aplica en la evaluación de posibles combinaciones para encontrar el equilibrio, evaluando las posibles combinaciones de estrategias, identificando las que en donde ninguna de las partes pueda mejorar su resultado cambiando su estrategia.\n",
    "\n",
    "\n",
    "\n",
    "5. Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma \n",
    "de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre \n",
    "exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la \n",
    "práctica? \n",
    "\n",
    "    El temporal difference learning se usa en varios campos, como en la robótica, gestión de recursos y optimización de procesos industriales. Los agentes usan el temporal difference learning para ajustar sus politicas de acción basándose en las recompensas recibidas. Entonces, esto permite una adaptación continua y mejora en la toma de desiciones, esto ayuda al entorno ya que son dinámicos. \n",
    "\n",
    "    Acerca de como maneja el temporal difference learning el equilibrio y explotación, es que este maneja el equilibrio por varias estrategias que permiten al agente aprender de manera efectiva en entornos dinámicos, como la estrategia de Epsilon-Greedy y Softmax.\n",
    "    Los desafíos que puede tener este es en el ajuste de parámetros, ya que determinar el valor óptimo de ε o los parámetros de la función softmax puede ser muy complicado y depende del entorno específico. También en el balance dinámico porque los entornos que cambian muy rápidamente, el equilibrio entre exploración y explotación puede necesitar ajustes contunios para irse adaptando a las nuevas condiciones. Por último, es importante asegurar que el agente explore lo suficiente para descubrir todas las posibles acciones y las consecuencias que tiene.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
