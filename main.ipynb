{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "integrantes: \n",
    "\n",
    "- Francis Aguilar #22243\n",
    "\n",
    "- Gerardo Pineda #22808\n",
    "\n",
    "- Angela Garcia #22869\n",
    "\n",
    "enlace al repositorio: https://github.com/angelargd8/LAB7-IA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Teor√≠a\n",
    "\n",
    "1. ¬øQu√© es el temporal difference learning y en qu√© se diferencia de los m√©todos tradicionales de aprendizaje \n",
    "supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de \n",
    "aprendizaje por refuerzo \n",
    "\n",
    "    El temporal difference learning es un m√©todo que combina las ideas del aprendizaje din√°mico y del aprendizaje supervisado.\n",
    "    Este tipo de aprendizaje esta basado en aprender a estimar valores futuros miestras va interactuando con un entorno, en vez de\n",
    "    esperar hasta el final de una secuencia para ajustar las predicciones. Lo que hace es actualizar gradualmente las estimaciones en \n",
    "    funci√≥n de las diferencias entre predicciones consecutivas. Entonces, las diferencias se usan para ajustar el modelo en tiempo real.\n",
    "    Acerca del concepto de error de diferencia temporal, esta en los algoritmos de aprendizaje por refuerzo, porque lo que mide es la discrepancia entre la recompensa esperada y la obtenida. \n",
    "\n",
    "\n",
    "2. En el contexto de los juegos simult√°neos, ¬øc√≥mo toman decisiones los jugadores sin conocer las acciones \n",
    "de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego \n",
    "simult√°neo y discuta las estrategias que los jugadores podr√≠an emplear en tal situaci√≥n \n",
    "\n",
    "    En los juegos simult√°neos un jugador al no conocer las acciones del oponente, busca estrategias considerando las posibles acciones del oponente, \n",
    "    pensando en obtener la mejor recompensa posible y muchas veces se toma en cuenta que al realizar una accion no dar pistas al oponente.\n",
    "    Los jugadores tambi√©n pueden tomar decisiones en funci√≥n de la informaci√≥n disponible, como por ejemplo, el estado del juego, \n",
    "    pero tambi√©n pueden tomar decisiones basadas en su conocimiento del entorno y de la estrategia de sus oponentes.\n",
    "    Un ejemplo del mundo real es el juego de werewolf en el que los jugadores no saben el rol de los dem√°s jugadores, y deben tomar decisiones\n",
    "    a base sin saber si son humanos o lobos, entonces tratan de tomar acciones que no demuestren que rol tienen y simular que todos son humanos, \n",
    "    las acciones en este caso son en las votaciones, porque al decir que alguien es el lobo tiene que tener sentido, ya que al no tenerlo lo pueden \n",
    "    juzgar que es el lobo y terminar muerto dentro del juego. Mientras que los lobos, deben de ser cautelosos al matar a alguien, ya que si se descubren los acusaran de ser el lobo y terminar√°n muertos.\n",
    "\n",
    "\n",
    "3. ¬øQu√© distingue los juegos de suma cero de los juegos de suma no cero y c√≥mo afecta esta diferencia al \n",
    "proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren \n",
    "en la categor√≠a de juegos de no suma cero y discuta las consideraciones estrat√©gicas √∫nicas involucradas \n",
    "\n",
    "    Lo que distingue a los juegos de suma cero de los juegos de suma de cero son en que los juegos de suma cero los \n",
    "    jugadores suelen actuar de una manera m√°s competitiva porque no hay beneficios que se puedan compartir.\n",
    "    Mientras, en los juegos de suma no cero los jugadores pueden buscar estrategias colaborativas porque hay beneficios que se pueden compartir.\n",
    "\n",
    "    Un ejemplo de los juegos de suma no cero es Genshin impact, porque en este se pueden hacer partidas colaborativas en donde \n",
    "    ambos jugadores obtengan beneficios, como por ejemplo, obtener recompensas, subir de nivel y objeto. Entonces, al entrar a alguna\n",
    "    mundo de otro jugador o cuando entran al mundo de uno, los jugadores pueden buscar una estrategia que maximice sus beneficios y minimice los beneficios de los dem√°s. Incluso otro ejemplo es minecraft en donde muchas veces se juega multijugador mientras uno cumple el rol de construir la casa, otro exploraci√≥n y otro minar, para obtener mayores recursos y compartirlos.\n",
    "\n",
    "\n",
    "\n",
    "4. ¬øC√≥mo se aplica el concepto de equilibrio de Nash a los juegos simult√°neos? Explicar c√≥mo el equilibrio de \n",
    "Nash representa una soluci√≥n estable en la que ning√∫n jugador tiene un incentivo para desviarse \n",
    "unilateralmente de la estrategia elegida \n",
    "\n",
    "    El concepto de equilibrio de Nash se aplica en los juegos simult√°neos porque los jugadores toman decisiones al mismo tiempo sin conocer las elecciones de los dem√°s. Las aplicaciones est√°n en la elecci√≥n de estrategias √≥timas cuando el jugador escoge su mejor respuesta para maximizar su utilidad a base de lo que los dem√°s est√°n haciendo. Tambi√©n, se aplica en la evaluaci√≥n de posibles combinaciones para encontrar el equilibrio, evaluando las posibles combinaciones de estrategias, identificando las que en donde ninguna de las partes pueda mejorar su resultado cambiando su estrategia.\n",
    "\n",
    "\n",
    "\n",
    "5. Discuta la aplicaci√≥n del temporal difference learning en el modelado y optimizaci√≥n de procesos de toma \n",
    "de decisiones en entornos din√°micos. ¬øC√≥mo maneja el temporal difference learning el equilibrio entre \n",
    "exploraci√≥n y explotaci√≥n y cu√°les son algunos de los desaf√≠os asociados con su implementaci√≥n en la \n",
    "pr√°ctica? \n",
    "\n",
    "    El temporal difference learning se usa en varios campos, como en la rob√≥tica, gesti√≥n de recursos y optimizaci√≥n de procesos industriales. Los agentes usan el temporal difference learning para ajustar sus politicas de acci√≥n bas√°ndose en las recompensas recibidas. Entonces, esto permite una adaptaci√≥n continua y mejora en la toma de desiciones, esto ayuda al entorno ya que son din√°micos. \n",
    "\n",
    "    Acerca de como maneja el temporal difference learning el equilibrio y explotaci√≥n, es que este maneja el equilibrio por varias estrategias que permiten al agente aprender de manera efectiva en entornos din√°micos, como la estrategia de Epsilon-Greedy y Softmax.\n",
    "    Los desaf√≠os que puede tener este es en el ajuste de par√°metros, ya que determinar el valor √≥ptimo de Œµ o los par√°metros de la funci√≥n softmax puede ser muy complicado y depende del entorno espec√≠fico. Tambi√©n en el balance din√°mico porque los entornos que cambian muy r√°pidamente, el equilibrio entre exploraci√≥n y explotaci√≥n puede necesitar ajustes contunios para irse adaptando a las nuevas condiciones. Por √∫ltimo, es importante asegurar que el agente explore lo suficiente para descubrir todas las posibles acciones y las consecuencias que tiene.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "class Conecta4:\n",
    "    FILAS = 6\n",
    "    COLUMNAS = 7\n",
    "    JUGADOR = 1\n",
    "    IA = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tablero = np.zeros((self.FILAS, self.COLUMNAS), dtype=int)  \n",
    "\n",
    "    def imprimir_tablero(self):\n",
    "        print(np.flip(self.tablero, 0)) \n",
    "\n",
    "    def es_movimiento_valido(self, columna):\n",
    "        return self.tablero[self.FILAS - 1, columna] == 0  \n",
    "\n",
    "    def obtener_fila_disponible(self, columna):\n",
    "        for fila in range(self.FILAS):\n",
    "            if self.tablero[fila, columna] == 0:\n",
    "                return fila\n",
    "        return None  \n",
    "\n",
    "    def realizar_movimiento(self, columna, jugador):\n",
    "        if not self.es_movimiento_valido(columna):\n",
    "            return False  \n",
    "        fila = self.obtener_fila_disponible(columna)\n",
    "        self.tablero[fila, columna] = jugador\n",
    "        return True\n",
    "\n",
    "    def verificar_victoria(self, jugador):\n",
    "        for fila in range(self.FILAS):\n",
    "            for col in range(self.COLUMNAS - 3):\n",
    "                if all(self.tablero[fila, col + i] == jugador for i in range(4)):\n",
    "                    return True\n",
    "\n",
    "        for col in range(self.COLUMNAS):\n",
    "            for fila in range(self.FILAS - 3):\n",
    "                if all(self.tablero[fila + i, col] == jugador for i in range(4)):\n",
    "                    return True\n",
    "\n",
    "        for fila in range(self.FILAS - 3):\n",
    "            for col in range(self.COLUMNAS - 3):\n",
    "                if all(self.tablero[fila + i, col + i] == jugador for i in range(4)):\n",
    "                    return True\n",
    "\n",
    "        for fila in range(3, self.FILAS):\n",
    "            for col in range(self.COLUMNAS - 3):\n",
    "                if all(self.tablero[fila - i, col + i] == jugador for i in range(4)):\n",
    "                    return True\n",
    "\n",
    "        return False  \n",
    "\n",
    "    def tablero_lleno(self):\n",
    "        return np.all(self.tablero != 0)\n",
    "\n",
    "    def obtener_movimientos_validos(self):\n",
    "        return [c for c in range(self.COLUMNAS) if self.es_movimiento_valido(c)]\n",
    "\n",
    "    def clonar_tablero(self):\n",
    "        nuevo = Conecta4()\n",
    "        nuevo.tablero = np.copy(self.tablero)\n",
    "        return nuevo\n",
    "\n",
    "# ====================== PRUEBA ======================\n",
    "if __name__ == \"__main__\":\n",
    "    tablero = Conecta4()\n",
    "    tablero.imprimir_tablero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle  # para guardar la Q-table\n",
    "\n",
    "class AgenteTD:\n",
    "    def __init__(self, jugador=2, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.jugador = jugador  # 1 o 2\n",
    "        self.Q = {}  # Q[(estado, acci√≥n)] = valor\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def obtener_estado(self, tablero):\n",
    "        \"\"\"Convierte el tablero a una tupla hashable\"\"\"\n",
    "        return tuple(tablero.flatten())\n",
    "\n",
    "    def elegir_accion(self, juego):\n",
    "        \"\"\"Estrategia Œµ-greedy\"\"\"\n",
    "        estado = self.obtener_estado(juego.tablero)\n",
    "        acciones = juego.obtener_movimientos_validos()\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(acciones)\n",
    "        \n",
    "        # Seleccionar acci√≥n con mayor Q\n",
    "        valores = [self.Q.get((estado, a), 0) for a in acciones]\n",
    "        max_q = max(valores)\n",
    "        mejores = [a for a, q in zip(acciones, valores) if q == max_q]\n",
    "        return random.choice(mejores)\n",
    "\n",
    "    def actualizar(self, estado, accion, recompensa, nuevo_estado, acciones_siguientes):\n",
    "        max_q_nuevo = max([self.Q.get((nuevo_estado, a), 0) for a in acciones_siguientes], default=0)\n",
    "        valor_actual = self.Q.get((estado, accion), 0)\n",
    "        self.Q[(estado, accion)] = valor_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo - valor_actual)\n",
    "\n",
    "    def guardar(self, archivo='qtable.pkl'):\n",
    "        with open(archivo, 'wb') as f:\n",
    "            pickle.dump(self.Q, f)\n",
    "\n",
    "    def cargar(self, archivo='qtable.pkl'):\n",
    "        try:\n",
    "            with open(archivo, 'rb') as f:\n",
    "                self.Q = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Q-table no encontrada. Se iniciar√° una vac√≠a.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tres_en_linea(tablero, jugador):\n",
    "    conteo = 0\n",
    "\n",
    "    # Horizontal\n",
    "    for fila in range(tablero.FILAS):\n",
    "        for col in range(tablero.COLUMNAS - 3):\n",
    "            ventana = tablero.tablero[fila, col:col+4]\n",
    "            if np.count_nonzero(ventana == jugador) == 3 and np.count_nonzero(ventana == 0) == 1:\n",
    "                conteo += 1\n",
    "\n",
    "    # Vertical\n",
    "    for col in range(tablero.COLUMNAS):\n",
    "        for fila in range(tablero.FILAS - 3):\n",
    "            ventana = tablero.tablero[fila:fila+4, col]\n",
    "            if np.count_nonzero(ventana == jugador) == 3 and np.count_nonzero(ventana == 0) == 1:\n",
    "                conteo += 1\n",
    "\n",
    "    # Diagonal positiva\n",
    "    for fila in range(tablero.FILAS - 3):\n",
    "        for col in range(tablero.COLUMNAS - 3):\n",
    "            ventana = [tablero.tablero[fila + i, col + i] for i in range(4)]\n",
    "            if ventana.count(jugador) == 3 and ventana.count(0) == 1:\n",
    "                conteo += 1\n",
    "\n",
    "    # Diagonal negativa\n",
    "    for fila in range(3, tablero.FILAS):\n",
    "        for col in range(tablero.COLUMNAS - 3):\n",
    "            ventana = [tablero.tablero[fila - i, col + i] for i in range(4)]\n",
    "            if ventana.count(jugador) == 3 and ventana.count(0) == 1:\n",
    "                conteo += 1\n",
    "\n",
    "    return conteo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1000 completado.\n",
      "Episodio 2000 completado.\n",
      "Episodio 3000 completado.\n",
      "Episodio 4000 completado.\n",
      "Episodio 5000 completado.\n",
      "Episodio 6000 completado.\n",
      "Episodio 7000 completado.\n",
      "Episodio 8000 completado.\n",
      "Episodio 9000 completado.\n",
      "Episodio 10000 completado.\n",
      "Episodio 11000 completado.\n",
      "Episodio 12000 completado.\n",
      "Episodio 13000 completado.\n",
      "Episodio 14000 completado.\n",
      "Episodio 15000 completado.\n",
      "Episodio 16000 completado.\n",
      "Episodio 17000 completado.\n",
      "Episodio 18000 completado.\n",
      "Episodio 19000 completado.\n",
      "Episodio 20000 completado.\n",
      "Episodio 21000 completado.\n",
      "Episodio 22000 completado.\n",
      "Episodio 23000 completado.\n",
      "Episodio 24000 completado.\n",
      "Episodio 25000 completado.\n",
      "Episodio 26000 completado.\n",
      "Episodio 27000 completado.\n",
      "Episodio 28000 completado.\n",
      "Episodio 29000 completado.\n",
      "Episodio 30000 completado.\n",
      "Episodio 31000 completado.\n",
      "Episodio 32000 completado.\n",
      "Episodio 33000 completado.\n",
      "Episodio 34000 completado.\n",
      "Episodio 35000 completado.\n",
      "Episodio 36000 completado.\n",
      "Episodio 37000 completado.\n",
      "Episodio 38000 completado.\n",
      "Episodio 39000 completado.\n",
      "Episodio 40000 completado.\n",
      "Episodio 41000 completado.\n",
      "Episodio 42000 completado.\n",
      "Episodio 43000 completado.\n",
      "Episodio 44000 completado.\n",
      "Episodio 45000 completado.\n",
      "Episodio 46000 completado.\n",
      "Episodio 47000 completado.\n",
      "Episodio 48000 completado.\n",
      "Episodio 49000 completado.\n",
      "Episodio 50000 completado.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del agente TD jugando contra un oponente aleatorio\n",
    "agente_td = AgenteTD(jugador=Conecta4.IA)\n",
    "oponente = Conecta4.JUGADOR\n",
    "\n",
    "episodios = 50000 \n",
    "\n",
    "for episodio in range(episodios):\n",
    "    juego = Conecta4()\n",
    "    estado = agente_td.obtener_estado(juego.tablero)\n",
    "    turno = agente_td.jugador\n",
    "\n",
    "    while not juego.tablero_lleno():\n",
    "        if turno == agente_td.jugador:\n",
    "            accion = agente_td.elegir_accion(juego)\n",
    "            juego.realizar_movimiento(accion, agente_td.jugador)\n",
    "\n",
    "            if juego.verificar_victoria(agente_td.jugador):\n",
    "                recompensa = 1\n",
    "                nuevo_estado = agente_td.obtener_estado(juego.tablero)\n",
    "                agente_td.actualizar(estado, accion, recompensa, nuevo_estado, [])\n",
    "                break\n",
    "\n",
    "            nuevo_estado = agente_td.obtener_estado(juego.tablero)\n",
    "            acciones_siguientes = juego.obtener_movimientos_validos()\n",
    "            agente_td.actualizar(estado, accion, 0, nuevo_estado, acciones_siguientes)\n",
    "            estado = nuevo_estado\n",
    "            turno = oponente\n",
    "\n",
    "        else:\n",
    "            accion_oponente = random.choice(juego.obtener_movimientos_validos())\n",
    "            juego.realizar_movimiento(accion_oponente, oponente)\n",
    "            if juego.verificar_victoria(oponente):\n",
    "                recompensa = -1\n",
    "                nuevo_estado = agente_td.obtener_estado(juego.tablero)\n",
    "                agente_td.actualizar(estado, accion, recompensa, nuevo_estado, [])\n",
    "                break\n",
    "            turno = agente_td.jugador\n",
    "\n",
    "    # Recompensa por empate si el tablero se llena y no hubo victoria\n",
    "    if juego.tablero_lleno() and not juego.verificar_victoria(agente_td.jugador) and not juego.verificar_victoria(oponente):\n",
    "        agente_td.actualizar(estado, accion, 0.5, agente_td.obtener_estado(juego.tablero), [])\n",
    "\n",
    "    if (episodio + 1) % 1000 == 0:\n",
    "        print(f\"Episodio {episodio + 1} completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenteIA:\n",
    "    def __init__(self, profundidad=4, poda_alpha_beta=True):\n",
    "        self.profundidad = profundidad\n",
    "        self.poda_alpha_beta = poda_alpha_beta\n",
    "\n",
    "    def evaluar_ventana(self, ventana, jugador):\n",
    "        puntuacion = 0\n",
    "        oponente = 3 - jugador  \n",
    "\n",
    "        # 4 en l√≠nea ‚Üí Victoria\n",
    "        if np.count_nonzero(ventana == jugador) == 4:  \n",
    "            puntuacion += 1000  \n",
    "        elif np.count_nonzero(ventana == jugador) == 3 and np.count_nonzero(ventana == 0) == 1:\n",
    "            puntuacion += 100  # Muy fuerte\n",
    "        elif np.count_nonzero(ventana == jugador) == 2 and np.count_nonzero(ventana == 0) == 2:\n",
    "            puntuacion += 10  # Buena jugada\n",
    "        elif np.count_nonzero(ventana == oponente) == 3 and np.count_nonzero(ventana == 0) == 1:\n",
    "            puntuacion -= 100  # Bloquear jugada peligrosa\n",
    "        elif np.count_nonzero(ventana == oponente) == 2 and np.count_nonzero(ventana == 0) == 2:\n",
    "            puntuacion -= 10  # Bloqueo medio\n",
    "\n",
    "        return puntuacion\n",
    "\n",
    "\n",
    "    def evaluar_posicion(self, tablero, jugador):\n",
    "        if tablero.verificar_victoria(jugador):\n",
    "            return 1000\n",
    "        elif tablero.verificar_victoria(3 - jugador):  \n",
    "            return -1000\n",
    "        else:\n",
    "            return random.randint(-10, 10)  \n",
    "\n",
    "    def minimax(self, tablero, profundidad, alpha, beta, maximizando):\n",
    "        if profundidad == 0 or tablero.verificar_victoria(Conecta4.JUGADOR) or tablero.verificar_victoria(Conecta4.IA) or tablero.tablero_lleno():\n",
    "            return self.evaluar_posicion(tablero, Conecta4.IA)\n",
    "\n",
    "        movimientos = tablero.obtener_movimientos_validos()\n",
    "\n",
    "        if maximizando:  \n",
    "            max_eval = -np.inf\n",
    "            for columna in movimientos:\n",
    "                tablero_copia = tablero.clonar_tablero()\n",
    "                tablero_copia.realizar_movimiento(columna, Conecta4.IA)\n",
    "                evaluacion = self.minimax(tablero_copia, profundidad - 1, alpha, beta, False)\n",
    "                max_eval = max(max_eval, evaluacion)\n",
    "\n",
    "                if self.poda_alpha_beta:\n",
    "                    alpha = max(alpha, evaluacion)\n",
    "                    if beta <= alpha:\n",
    "                        break  \n",
    "            return max_eval\n",
    "        else:  \n",
    "            min_eval = np.inf\n",
    "            for columna in movimientos:\n",
    "                tablero_copia = tablero.clonar_tablero()\n",
    "                tablero_copia.realizar_movimiento(columna, Conecta4.JUGADOR)\n",
    "                evaluacion = self.minimax(tablero_copia, profundidad - 1, alpha, beta, True)\n",
    "                min_eval = min(min_eval, evaluacion)\n",
    "\n",
    "                if self.poda_alpha_beta:\n",
    "                    beta = min(beta, evaluacion)\n",
    "                    if beta <= alpha:\n",
    "                        break  \n",
    "            return min_eval\n",
    "\n",
    "    def mejor_movimiento(self, tablero):\n",
    "        \"\"\" Calcula el mejor movimiento disponible usando Minimax \"\"\"\n",
    "        mejor_columna = random.choice(tablero.obtener_movimientos_validos())  \n",
    "        mejor_valor = -np.inf\n",
    "        alpha, beta = -np.inf, np.inf\n",
    "\n",
    "        for columna in tablero.obtener_movimientos_validos():\n",
    "            tablero_copia = tablero.clonar_tablero()\n",
    "            tablero_copia.realizar_movimiento(columna, Conecta4.IA)\n",
    "            valor_movimiento = self.minimax(tablero_copia, self.profundidad, alpha, beta, False)\n",
    "\n",
    "            if valor_movimiento > mejor_valor:\n",
    "                mejor_valor = valor_movimiento\n",
    "                mejor_columna = columna\n",
    "\n",
    "        return mejor_columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jugar_partida(agente1, agente2, mostrar=False):\n",
    "    juego = Conecta4()\n",
    "    jugador_actual = Conecta4.JUGADOR  # Agente 1\n",
    "\n",
    "    while not juego.tablero_lleno():\n",
    "        if mostrar:\n",
    "            juego.imprimir_tablero()\n",
    "\n",
    "        if jugador_actual == Conecta4.JUGADOR:\n",
    "            columna = agente1.mejor_movimiento(juego) if hasattr(agente1, 'mejor_movimiento') else agente1.elegir_accion(juego)\n",
    "            juego.realizar_movimiento(columna, Conecta4.JUGADOR)\n",
    "            if juego.verificar_victoria(Conecta4.JUGADOR):\n",
    "                return 1  # Gana agente1\n",
    "            jugador_actual = Conecta4.IA\n",
    "        else:\n",
    "            columna = agente2.mejor_movimiento(juego) if hasattr(agente2, 'mejor_movimiento') else agente2.elegir_accion(juego)\n",
    "            juego.realizar_movimiento(columna, Conecta4.IA)\n",
    "            if juego.verificar_victoria(Conecta4.IA):\n",
    "                return 2  # Gana agente2\n",
    "            jugador_actual = Conecta4.JUGADOR\n",
    "\n",
    "    return 0  # Empate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simular_enfrentamientos(agente1, agente2, n=50):\n",
    "    resultados = {1: 0, 2: 0, 0: 0}  # {agente1, agente2, empate}\n",
    "    for i in range(n):\n",
    "        resultado = jugar_partida(agente1, agente2)\n",
    "        resultados[resultado] += 1\n",
    "        print(f\"Partida {i + 1}: {resultado}\")\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ ¬°Empieza el juego! T√∫ eres el jugador 1\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]]\n",
      "ü§ñ Turno del agente...\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0]]\n",
      "ü§ñ Turno del agente...\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 0]]\n",
      "ü§ñ Turno del agente...\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 1 0 1 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 1 0 1 0]]\n",
      "üéâ ¬°Has ganado!\n"
     ]
    }
   ],
   "source": [
    "def jugar_contra_td(agente_td, jugador_humano=1):\n",
    "    juego = Conecta4()\n",
    "    turno = jugador_humano  # Comienza el humano si es 1, de lo contrario inicia el agente\n",
    "\n",
    "    print(\"üéÆ ¬°Empieza el juego! T√∫ eres el jugador\", jugador_humano)\n",
    "    juego.imprimir_tablero()\n",
    "\n",
    "    while not juego.tablero_lleno():\n",
    "        if turno == jugador_humano:\n",
    "            try:\n",
    "                columna = int(input(\"Selecciona una columna (0-6): \"))\n",
    "                if columna < 0 or columna >= juego.COLUMNAS:\n",
    "                    print(\"‚ùå Columna inv√°lida.\")\n",
    "                    continue\n",
    "                if not juego.realizar_movimiento(columna, jugador_humano):\n",
    "                    print(\"‚ùå Columna llena.\")\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                print(\"‚ö†Ô∏è Ingresa un n√∫mero v√°lido.\")\n",
    "                continue\n",
    "\n",
    "            if juego.verificar_victoria(jugador_humano):\n",
    "                juego.imprimir_tablero()\n",
    "                print(\"üéâ ¬°Has ganado!\")\n",
    "                return\n",
    "\n",
    "            turno = 3 - jugador_humano  # Cambia de turno\n",
    "        else:\n",
    "            print(\"ü§ñ Turno del agente...\")\n",
    "            columna = agente_td.elegir_accion(juego)\n",
    "            juego.realizar_movimiento(columna, agente_td.jugador)\n",
    "            if juego.verificar_victoria(agente_td.jugador):\n",
    "                juego.imprimir_tablero()\n",
    "                print(\"üíª El agente ha ganado.\")\n",
    "                return\n",
    "\n",
    "            turno = jugador_humano\n",
    "\n",
    "        juego.imprimir_tablero()\n",
    "\n",
    "    print(\"ü§ù ¬°Empate!\")\n",
    "\n",
    "\n",
    "jugar_contra_td(agente_td)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partida 1: 2\n",
      "Partida 2: 2\n",
      "Partida 3: 2\n",
      "Partida 4: 2\n",
      "Partida 5: 2\n",
      "Partida 6: 2\n",
      "Partida 7: 2\n",
      "Partida 8: 2\n",
      "Partida 9: 2\n",
      "Partida 10: 2\n",
      "Partida 11: 2\n",
      "Partida 12: 2\n",
      "Partida 13: 2\n",
      "Partida 14: 2\n",
      "Partida 15: 2\n",
      "Partida 16: 2\n",
      "Partida 17: 2\n",
      "Partida 18: 2\n",
      "Partida 19: 2\n",
      "Partida 20: 2\n",
      "Partida 21: 2\n",
      "Partida 22: 2\n",
      "Partida 23: 2\n",
      "Partida 24: 2\n",
      "Partida 25: 2\n",
      "Partida 26: 2\n",
      "Partida 27: 2\n",
      "Partida 28: 2\n",
      "Partida 29: 2\n",
      "Partida 30: 2\n",
      "Partida 31: 2\n",
      "Partida 32: 2\n",
      "Partida 33: 2\n",
      "Partida 34: 2\n",
      "Partida 35: 2\n",
      "Partida 36: 2\n",
      "Partida 37: 2\n",
      "Partida 38: 2\n",
      "Partida 39: 2\n",
      "Partida 40: 2\n",
      "Partida 41: 2\n",
      "Partida 42: 2\n",
      "Partida 43: 2\n",
      "Partida 44: 2\n",
      "Partida 45: 2\n",
      "Partida 46: 2\n",
      "Partida 47: 2\n",
      "Partida 48: 2\n",
      "Partida 49: 2\n",
      "Partida 50: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Esto es IA vs IA (NO EJECUTAR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# agente_td = AgenteTD(jugador=Conecta4.JUGADOR)  # Entrenado antes\n",
    "agente_minimax = AgenteIA(profundidad=4, poda_alpha_beta=False)\n",
    "agente_minimax_ab = AgenteIA(profundidad=4, poda_alpha_beta=True)\n",
    "\n",
    "# TD vs Minimax\n",
    "# res_td_vs_min = simular_enfrentamientos(agente, agente_minimax)\n",
    "\n",
    "# TD vs Minimax con poda\n",
    "res_td_vs_ab = simular_enfrentamientos(agente_td, agente_minimax_ab)\n",
    "\n",
    "# Minimax vs Minimax con poda\n",
    "# res_min_vs_ab = simular_enfrentamientos(agente_minimax, agente_minimax_ab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = ['Agente TD', 'Minimax', 'Minimax AB']\n",
    "victorias = [\n",
    "    res_td_vs_min[1],\n",
    "    res_td_vs_min[2],\n",
    "    0\n",
    "]\n",
    "\n",
    "victorias[0] += res_td_vs_ab[1]\n",
    "victorias[2] += res_td_vs_ab[2]\n",
    "victorias[1] += res_min_vs_ab[1]\n",
    "victorias[2] += res_min_vs_ab[2]\n",
    "\n",
    "plt.bar(labels, victorias)\n",
    "plt.title(\"Victorias de los Agentes en 150 partidas\")\n",
    "plt.ylabel(\"Cantidad de Victorias\")\n",
    "plt.savefig(\"resultados_agentes.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
