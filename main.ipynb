{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "integrantes: \n",
    "\n",
    "- Francis Aguilar #22243\n",
    "\n",
    "- Gerardo Pineda #22808\n",
    "\n",
    "- Angela Garcia #22869\n",
    "\n",
    "enlace al repositorio: https://github.com/angelargd8/LAB7-IA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Teoría\n",
    "\n",
    "1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje \n",
    "supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de \n",
    "aprendizaje por refuerzo \n",
    "\n",
    "    El temporal difference learning es un método que combina las ideas del aprendizaje dinámico y del aprendizaje supervisado.\n",
    "    Este tipo de aprendizaje esta basado en aprender a estimar valores futuros miestras va interactuando con un entorno, en vez de\n",
    "    esperar hasta el final de una secuencia para ajustar las predicciones. Lo que hace es actualizar gradualmente las estimaciones en \n",
    "    función de las diferencias entre predicciones consecutivas. Entonces, las diferencias se usan para ajustar el modelo en tiempo real.\n",
    "    Acerca del concepto de error de diferencia temporal, esta en los algoritmos de aprendizaje por refuerzo, porque lo que mide es la discrepancia entre la recompensa esperada y la obtenida. \n",
    "\n",
    "\n",
    "2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones \n",
    "de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego \n",
    "simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación \n",
    "\n",
    "    En los juegos simultáneos un jugador al no conocer las acciones del oponente, busca estrategias considerando las posibles acciones del oponente, \n",
    "    pensando en obtener la mejor recompensa posible y muchas veces se toma en cuenta que al realizar una accion no dar pistas al oponente.\n",
    "    Los jugadores también pueden tomar decisiones en función de la información disponible, como por ejemplo, el estado del juego, \n",
    "    pero también pueden tomar decisiones basadas en su conocimiento del entorno y de la estrategia de sus oponentes.\n",
    "    Un ejemplo del mundo real es el juego de werewolf en el que los jugadores no saben el rol de los demás jugadores, y deben tomar decisiones\n",
    "    a base sin saber si son humanos o lobos, entonces tratan de tomar acciones que no demuestren que rol tienen y simular que todos son humanos, \n",
    "    las acciones en este caso son en las votaciones, porque al decir que alguien es el lobo tiene que tener sentido, ya que al no tenerlo lo pueden \n",
    "    juzgar que es el lobo y terminar muerto dentro del juego. Mientras que los lobos, deben de ser cautelosos al matar a alguien, ya que si se descubren los acusaran de ser el lobo y terminarán muertos.\n",
    "\n",
    "\n",
    "3. ¿Qué distingue los juegos de suma cero de los juegos de suma no cero y cómo afecta esta diferencia al \n",
    "proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren \n",
    "en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas \n",
    "\n",
    "    Lo que distingue a los juegos de suma cero de los juegos de suma de cero son en que los juegos de suma cero los \n",
    "    jugadores suelen actuar de una manera más competitiva porque no hay beneficios que se puedan compartir.\n",
    "    Mientras, en los juegos de suma no cero los jugadores pueden buscar estrategias colaborativas porque hay beneficios que se pueden compartir.\n",
    "\n",
    "    Un ejemplo de los juegos de suma no cero es Genshin impact, porque en este se pueden hacer partidas colaborativas en donde \n",
    "    ambos jugadores obtengan beneficios, como por ejemplo, obtener recompensas, subir de nivel y objeto. Entonces, al entrar a alguna\n",
    "    mundo de otro jugador o cuando entran al mundo de uno, los jugadores pueden buscar una estrategia que maximice sus beneficios y minimice los beneficios de los demás. Incluso otro ejemplo es minecraft en donde muchas veces se juega multijugador mientras uno cumple el rol de construir la casa, otro exploración y otro minar, para obtener mayores recursos y compartirlos.\n",
    "\n",
    "\n",
    "\n",
    "4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de \n",
    "Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse \n",
    "unilateralmente de la estrategia elegida \n",
    "\n",
    "    El concepto de equilibrio de Nash se aplica en los juegos simultáneos porque los jugadores toman decisiones al mismo tiempo sin conocer las elecciones de los demás. Las aplicaciones están en la elección de estrategias ótimas cuando el jugador escoge su mejor respuesta para maximizar su utilidad a base de lo que los demás están haciendo. También, se aplica en la evaluación de posibles combinaciones para encontrar el equilibrio, evaluando las posibles combinaciones de estrategias, identificando las que en donde ninguna de las partes pueda mejorar su resultado cambiando su estrategia.\n",
    "\n",
    "\n",
    "\n",
    "5. Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma \n",
    "de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre \n",
    "exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la \n",
    "práctica? \n",
    "\n",
    "    El temporal difference learning se usa en varios campos, como en la robótica, gestión de recursos y optimización de procesos industriales. Los agentes usan el temporal difference learning para ajustar sus politicas de acción basándose en las recompensas recibidas. Entonces, esto permite una adaptación continua y mejora en la toma de desiciones, esto ayuda al entorno ya que son dinámicos. \n",
    "\n",
    "    Acerca de como maneja el temporal difference learning el equilibrio y explotación, es que este maneja el equilibrio por varias estrategias que permiten al agente aprender de manera efectiva en entornos dinámicos, como la estrategia de Epsilon-Greedy y Softmax.\n",
    "    Los desafíos que puede tener este es en el ajuste de parámetros, ya que determinar el valor óptimo de ε o los parámetros de la función softmax puede ser muy complicado y depende del entorno específico. También en el balance dinámico porque los entornos que cambian muy rápidamente, el equilibrio entre exploración y explotación puede necesitar ajustes contunios para irse adaptando a las nuevas condiciones. Por último, es importante asegurar que el agente explore lo suficiente para descubrir todas las posibles acciones y las consecuencias que tiene.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "class Conecta4:\n",
    "    FILAS = 6\n",
    "    COLUMNAS = 7\n",
    "    JUGADOR = 1\n",
    "    IA = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tablero = np.zeros((self.FILAS, self.COLUMNAS), dtype=int)  \n",
    "\n",
    "    def imprimir_tablero(self):\n",
    "        print(np.flip(self.tablero, 0)) \n",
    "\n",
    "    def es_movimiento_valido(self, columna):\n",
    "        return self.tablero[self.FILAS - 1, columna] == 0  \n",
    "\n",
    "    def obtener_fila_disponible(self, columna):\n",
    "        for fila in range(self.FILAS):\n",
    "            if self.tablero[fila, columna] == 0:\n",
    "                return fila\n",
    "        return None  \n",
    "\n",
    "    def realizar_movimiento(self, columna, jugador):\n",
    "        if not self.es_movimiento_valido(columna):\n",
    "            return False  \n",
    "        fila = self.obtener_fila_disponible(columna)\n",
    "        self.tablero[fila, columna] = jugador\n",
    "        return True\n",
    "\n",
    "    def verificar_victoria(self, jugador):\n",
    "        for fila in range(self.FILAS):\n",
    "            for col in range(self.COLUMNAS - 3):\n",
    "                if all(self.tablero[fila, col + i] == jugador for i in range(4)):\n",
    "                    return True\n",
    "\n",
    "        for col in range(self.COLUMNAS):\n",
    "            for fila in range(self.FILAS - 3):\n",
    "                if all(self.tablero[fila + i, col] == jugador for i in range(4)):\n",
    "                    return True\n",
    "\n",
    "        for fila in range(self.FILAS - 3):\n",
    "            for col in range(self.COLUMNAS - 3):\n",
    "                if all(self.tablero[fila + i, col + i] == jugador for i in range(4)):\n",
    "                    return True\n",
    "\n",
    "        for fila in range(3, self.FILAS):\n",
    "            for col in range(self.COLUMNAS - 3):\n",
    "                if all(self.tablero[fila - i, col + i] == jugador for i in range(4)):\n",
    "                    return True\n",
    "\n",
    "        return False  \n",
    "\n",
    "    def tablero_lleno(self):\n",
    "        return np.all(self.tablero != 0)\n",
    "\n",
    "    def obtener_movimientos_validos(self):\n",
    "        return [c for c in range(self.COLUMNAS) if self.es_movimiento_valido(c)]\n",
    "\n",
    "    def clonar_tablero(self):\n",
    "        nuevo = Conecta4()\n",
    "        nuevo.tablero = np.copy(self.tablero)\n",
    "        return nuevo\n",
    "\n",
    "# ====================== PRUEBA ======================\n",
    "if __name__ == \"__main__\":\n",
    "    tablero = Conecta4()\n",
    "    tablero.imprimir_tablero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle  # para guardar la Q-table\n",
    "\n",
    "class AgenteTD:\n",
    "    def __init__(self, jugador=2, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.jugador = jugador  # 1 o 2\n",
    "        self.Q = {}  # Q[(estado, acción)] = valor\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def obtener_estado(self, tablero):\n",
    "        \"\"\"Convierte el tablero a una tupla hashable\"\"\"\n",
    "        return tuple(tablero.flatten())\n",
    "\n",
    "    def elegir_accion(self, juego):\n",
    "        \"\"\"Estrategia ε-greedy\"\"\"\n",
    "        estado = self.obtener_estado(juego.tablero)\n",
    "        acciones = juego.obtener_movimientos_validos()\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(acciones)\n",
    "        \n",
    "        # Seleccionar acción con mayor Q\n",
    "        valores = [self.Q.get((estado, a), 0) for a in acciones]\n",
    "        max_q = max(valores)\n",
    "        mejores = [a for a, q in zip(acciones, valores) if q == max_q]\n",
    "        return random.choice(mejores)\n",
    "\n",
    "    def actualizar(self, estado, accion, recompensa, nuevo_estado, acciones_siguientes):\n",
    "        max_q_nuevo = max([self.Q.get((nuevo_estado, a), 0) for a in acciones_siguientes], default=0)\n",
    "        valor_actual = self.Q.get((estado, accion), 0)\n",
    "        self.Q[(estado, accion)] = valor_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo - valor_actual)\n",
    "\n",
    "    def guardar(self, archivo='qtable.pkl'):\n",
    "        with open(archivo, 'wb') as f:\n",
    "            pickle.dump(self.Q, f)\n",
    "\n",
    "    def cargar(self, archivo='qtable.pkl'):\n",
    "        try:\n",
    "            with open(archivo, 'rb') as f:\n",
    "                self.Q = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Q-table no encontrada. Se iniciará una vacía.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tres_en_linea(tablero, jugador):\n",
    "    conteo = 0\n",
    "\n",
    "    # Horizontal\n",
    "    for fila in range(tablero.FILAS):\n",
    "        for col in range(tablero.COLUMNAS - 3):\n",
    "            ventana = tablero.tablero[fila, col:col+4]\n",
    "            if np.count_nonzero(ventana == jugador) == 3 and np.count_nonzero(ventana == 0) == 1:\n",
    "                conteo += 1\n",
    "\n",
    "    # Vertical\n",
    "    for col in range(tablero.COLUMNAS):\n",
    "        for fila in range(tablero.FILAS - 3):\n",
    "            ventana = tablero.tablero[fila:fila+4, col]\n",
    "            if np.count_nonzero(ventana == jugador) == 3 and np.count_nonzero(ventana == 0) == 1:\n",
    "                conteo += 1\n",
    "\n",
    "    # Diagonal positiva\n",
    "    for fila in range(tablero.FILAS - 3):\n",
    "        for col in range(tablero.COLUMNAS - 3):\n",
    "            ventana = [tablero.tablero[fila + i, col + i] for i in range(4)]\n",
    "            if ventana.count(jugador) == 3 and ventana.count(0) == 1:\n",
    "                conteo += 1\n",
    "\n",
    "    # Diagonal negativa\n",
    "    for fila in range(3, tablero.FILAS):\n",
    "        for col in range(tablero.COLUMNAS - 3):\n",
    "            ventana = [tablero.tablero[fila - i, col + i] for i in range(4)]\n",
    "            if ventana.count(jugador) == 3 and ventana.count(0) == 1:\n",
    "                conteo += 1\n",
    "\n",
    "    return conteo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1000 completado.\n",
      "Episodio 2000 completado.\n",
      "Episodio 3000 completado.\n",
      "Episodio 4000 completado.\n",
      "Episodio 5000 completado.\n",
      "Episodio 6000 completado.\n",
      "Episodio 7000 completado.\n",
      "Episodio 8000 completado.\n",
      "Episodio 9000 completado.\n",
      "Episodio 10000 completado.\n",
      "Episodio 11000 completado.\n",
      "Episodio 12000 completado.\n",
      "Episodio 13000 completado.\n",
      "Episodio 14000 completado.\n",
      "Episodio 15000 completado.\n",
      "Episodio 16000 completado.\n",
      "Episodio 17000 completado.\n",
      "Episodio 18000 completado.\n",
      "Episodio 19000 completado.\n",
      "Episodio 20000 completado.\n",
      "Episodio 21000 completado.\n",
      "Episodio 22000 completado.\n",
      "Episodio 23000 completado.\n",
      "Episodio 24000 completado.\n",
      "Episodio 25000 completado.\n",
      "Episodio 26000 completado.\n",
      "Episodio 27000 completado.\n",
      "Episodio 28000 completado.\n",
      "Episodio 29000 completado.\n",
      "Episodio 30000 completado.\n",
      "Episodio 31000 completado.\n",
      "Episodio 32000 completado.\n",
      "Episodio 33000 completado.\n",
      "Episodio 34000 completado.\n",
      "Episodio 35000 completado.\n",
      "Episodio 36000 completado.\n",
      "Episodio 37000 completado.\n",
      "Episodio 38000 completado.\n",
      "Episodio 39000 completado.\n",
      "Episodio 40000 completado.\n",
      "Episodio 41000 completado.\n",
      "Episodio 42000 completado.\n",
      "Episodio 43000 completado.\n",
      "Episodio 44000 completado.\n",
      "Episodio 45000 completado.\n",
      "Episodio 46000 completado.\n",
      "Episodio 47000 completado.\n",
      "Episodio 48000 completado.\n",
      "Episodio 49000 completado.\n",
      "Episodio 50000 completado.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del agente TD jugando contra un oponente aleatorio\n",
    "agente_td = AgenteTD(jugador=Conecta4.IA)\n",
    "oponente = Conecta4.JUGADOR\n",
    "\n",
    "episodios = 50000 \n",
    "\n",
    "for episodio in range(episodios):\n",
    "    juego = Conecta4()\n",
    "    estado = agente_td.obtener_estado(juego.tablero)\n",
    "    turno = agente_td.jugador\n",
    "\n",
    "    while not juego.tablero_lleno():\n",
    "        if turno == agente_td.jugador:\n",
    "            accion = agente_td.elegir_accion(juego)\n",
    "            juego.realizar_movimiento(accion, agente_td.jugador)\n",
    "\n",
    "            if juego.verificar_victoria(agente_td.jugador):\n",
    "                recompensa = 1\n",
    "                nuevo_estado = agente_td.obtener_estado(juego.tablero)\n",
    "                agente_td.actualizar(estado, accion, recompensa, nuevo_estado, [])\n",
    "                break\n",
    "\n",
    "            nuevo_estado = agente_td.obtener_estado(juego.tablero)\n",
    "            acciones_siguientes = juego.obtener_movimientos_validos()\n",
    "            agente_td.actualizar(estado, accion, 0, nuevo_estado, acciones_siguientes)\n",
    "            estado = nuevo_estado\n",
    "            turno = oponente\n",
    "\n",
    "        else:\n",
    "            accion_oponente = random.choice(juego.obtener_movimientos_validos())\n",
    "            juego.realizar_movimiento(accion_oponente, oponente)\n",
    "            if juego.verificar_victoria(oponente):\n",
    "                recompensa = -1\n",
    "                nuevo_estado = agente_td.obtener_estado(juego.tablero)\n",
    "                agente_td.actualizar(estado, accion, recompensa, nuevo_estado, [])\n",
    "                break\n",
    "            turno = agente_td.jugador\n",
    "\n",
    "    # Recompensa por empate si el tablero se llena y no hubo victoria\n",
    "    if juego.tablero_lleno() and not juego.verificar_victoria(agente_td.jugador) and not juego.verificar_victoria(oponente):\n",
    "        agente_td.actualizar(estado, accion, 0.5, agente_td.obtener_estado(juego.tablero), [])\n",
    "\n",
    "    if (episodio + 1) % 1000 == 0:\n",
    "        print(f\"Episodio {episodio + 1} completado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenteIA:\n",
    "    def __init__(self, profundidad=4, poda_alpha_beta=True):\n",
    "        self.profundidad = profundidad\n",
    "        self.poda_alpha_beta = poda_alpha_beta\n",
    "\n",
    "    def evaluar_ventana(self, ventana, jugador):\n",
    "        puntuacion = 0\n",
    "        oponente = 3 - jugador  \n",
    "\n",
    "        # 4 en línea → Victoria\n",
    "        if np.count_nonzero(ventana == jugador) == 4:  \n",
    "            puntuacion += 1000  \n",
    "        elif np.count_nonzero(ventana == jugador) == 3 and np.count_nonzero(ventana == 0) == 1:\n",
    "            puntuacion += 100  # Muy fuerte\n",
    "        elif np.count_nonzero(ventana == jugador) == 2 and np.count_nonzero(ventana == 0) == 2:\n",
    "            puntuacion += 10  # Buena jugada\n",
    "        elif np.count_nonzero(ventana == oponente) == 3 and np.count_nonzero(ventana == 0) == 1:\n",
    "            puntuacion -= 100  # Bloquear jugada peligrosa\n",
    "        elif np.count_nonzero(ventana == oponente) == 2 and np.count_nonzero(ventana == 0) == 2:\n",
    "            puntuacion -= 10  # Bloqueo medio\n",
    "\n",
    "        return puntuacion\n",
    "\n",
    "\n",
    "    def evaluar_posicion(self, tablero, jugador):\n",
    "        if tablero.verificar_victoria(jugador):\n",
    "            return 1000\n",
    "        elif tablero.verificar_victoria(3 - jugador):  \n",
    "            return -1000\n",
    "        else:\n",
    "            return random.randint(-10, 10)  \n",
    "\n",
    "    def minimax(self, tablero, profundidad, alpha, beta, maximizando):\n",
    "        if profundidad == 0 or tablero.verificar_victoria(Conecta4.JUGADOR) or tablero.verificar_victoria(Conecta4.IA) or tablero.tablero_lleno():\n",
    "            return self.evaluar_posicion(tablero, Conecta4.IA)\n",
    "\n",
    "        movimientos = tablero.obtener_movimientos_validos()\n",
    "\n",
    "        if maximizando:  \n",
    "            max_eval = -np.inf\n",
    "            for columna in movimientos:\n",
    "                tablero_copia = tablero.clonar_tablero()\n",
    "                tablero_copia.realizar_movimiento(columna, Conecta4.IA)\n",
    "                evaluacion = self.minimax(tablero_copia, profundidad - 1, alpha, beta, False)\n",
    "                max_eval = max(max_eval, evaluacion)\n",
    "\n",
    "                if self.poda_alpha_beta:\n",
    "                    alpha = max(alpha, evaluacion)\n",
    "                    if beta <= alpha:\n",
    "                        break  \n",
    "            return max_eval\n",
    "        else:  \n",
    "            min_eval = np.inf\n",
    "            for columna in movimientos:\n",
    "                tablero_copia = tablero.clonar_tablero()\n",
    "                tablero_copia.realizar_movimiento(columna, Conecta4.JUGADOR)\n",
    "                evaluacion = self.minimax(tablero_copia, profundidad - 1, alpha, beta, True)\n",
    "                min_eval = min(min_eval, evaluacion)\n",
    "\n",
    "                if self.poda_alpha_beta:\n",
    "                    beta = min(beta, evaluacion)\n",
    "                    if beta <= alpha:\n",
    "                        break  \n",
    "            return min_eval\n",
    "\n",
    "    def mejor_movimiento(self, tablero):\n",
    "        \"\"\" Calcula el mejor movimiento disponible usando Minimax \"\"\"\n",
    "        mejor_columna = random.choice(tablero.obtener_movimientos_validos())  \n",
    "        mejor_valor = -np.inf\n",
    "        alpha, beta = -np.inf, np.inf\n",
    "\n",
    "        for columna in tablero.obtener_movimientos_validos():\n",
    "            tablero_copia = tablero.clonar_tablero()\n",
    "            tablero_copia.realizar_movimiento(columna, Conecta4.IA)\n",
    "            valor_movimiento = self.minimax(tablero_copia, self.profundidad, alpha, beta, False)\n",
    "\n",
    "            if valor_movimiento > mejor_valor:\n",
    "                mejor_valor = valor_movimiento\n",
    "                mejor_columna = columna\n",
    "\n",
    "        return mejor_columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jugar_partida(agente1, agente2, mostrar=False):\n",
    "    juego = Conecta4()\n",
    "    jugador_actual = Conecta4.JUGADOR  # Agente 1\n",
    "\n",
    "    while not juego.tablero_lleno():\n",
    "        if mostrar:\n",
    "            juego.imprimir_tablero()\n",
    "\n",
    "        if jugador_actual == Conecta4.JUGADOR:\n",
    "            columna = agente1.mejor_movimiento(juego) if hasattr(agente1, 'mejor_movimiento') else agente1.elegir_accion(juego)\n",
    "            juego.realizar_movimiento(columna, Conecta4.JUGADOR)\n",
    "            if juego.verificar_victoria(Conecta4.JUGADOR):\n",
    "                return 1  # Gana agente1\n",
    "            jugador_actual = Conecta4.IA\n",
    "        else:\n",
    "            columna = agente2.mejor_movimiento(juego) if hasattr(agente2, 'mejor_movimiento') else agente2.elegir_accion(juego)\n",
    "            juego.realizar_movimiento(columna, Conecta4.IA)\n",
    "            if juego.verificar_victoria(Conecta4.IA):\n",
    "                return 2  # Gana agente2\n",
    "            jugador_actual = Conecta4.JUGADOR\n",
    "\n",
    "    return 0  # Empate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simular_enfrentamientos(agente1, agente2, n=50):\n",
    "    resultados = {1: 0, 2: 0, 0: 0}  # {agente1, agente2, empate}\n",
    "    for i in range(n):\n",
    "        resultado = jugar_partida(agente1, agente2)\n",
    "        resultados[resultado] += 1\n",
    "        print(f\"Partida {i + 1}: {resultado}\")\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 ¡Empieza el juego! Tú eres el jugador 1\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]]\n",
      "🤖 Turno del agente...\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0]]\n",
      "🤖 Turno del agente...\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 0]]\n",
      "🤖 Turno del agente...\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 1 0 1 0]]\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 1 0 1 0]]\n",
      "🎉 ¡Has ganado!\n"
     ]
    }
   ],
   "source": [
    "def jugar_contra_td(agente_td, jugador_humano=1):\n",
    "    juego = Conecta4()\n",
    "    turno = jugador_humano  # Comienza el humano si es 1, de lo contrario inicia el agente\n",
    "\n",
    "    print(\"🎮 ¡Empieza el juego! Tú eres el jugador\", jugador_humano)\n",
    "    juego.imprimir_tablero()\n",
    "\n",
    "    while not juego.tablero_lleno():\n",
    "        if turno == jugador_humano:\n",
    "            try:\n",
    "                columna = int(input(\"Selecciona una columna (0-6): \"))\n",
    "                if columna < 0 or columna >= juego.COLUMNAS:\n",
    "                    print(\"❌ Columna inválida.\")\n",
    "                    continue\n",
    "                if not juego.realizar_movimiento(columna, jugador_humano):\n",
    "                    print(\"❌ Columna llena.\")\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                print(\"⚠️ Ingresa un número válido.\")\n",
    "                continue\n",
    "\n",
    "            if juego.verificar_victoria(jugador_humano):\n",
    "                juego.imprimir_tablero()\n",
    "                print(\"🎉 ¡Has ganado!\")\n",
    "                return\n",
    "\n",
    "            turno = 3 - jugador_humano  # Cambia de turno\n",
    "        else:\n",
    "            print(\"🤖 Turno del agente...\")\n",
    "            columna = agente_td.elegir_accion(juego)\n",
    "            juego.realizar_movimiento(columna, agente_td.jugador)\n",
    "            if juego.verificar_victoria(agente_td.jugador):\n",
    "                juego.imprimir_tablero()\n",
    "                print(\"💻 El agente ha ganado.\")\n",
    "                return\n",
    "\n",
    "            turno = jugador_humano\n",
    "\n",
    "        juego.imprimir_tablero()\n",
    "\n",
    "    print(\"🤝 ¡Empate!\")\n",
    "\n",
    "\n",
    "jugar_contra_td(agente_td)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partida 1: 2\n",
      "Partida 2: 2\n",
      "Partida 3: 2\n",
      "Partida 4: 2\n",
      "Partida 5: 2\n",
      "Partida 6: 2\n",
      "Partida 7: 2\n",
      "Partida 8: 2\n",
      "Partida 9: 2\n",
      "Partida 10: 2\n",
      "Partida 11: 2\n",
      "Partida 12: 2\n",
      "Partida 13: 2\n",
      "Partida 14: 2\n",
      "Partida 15: 2\n",
      "Partida 16: 2\n",
      "Partida 17: 2\n",
      "Partida 18: 2\n",
      "Partida 19: 2\n",
      "Partida 20: 2\n",
      "Partida 21: 2\n",
      "Partida 22: 2\n",
      "Partida 23: 2\n",
      "Partida 24: 2\n",
      "Partida 25: 2\n",
      "Partida 26: 2\n",
      "Partida 27: 2\n",
      "Partida 28: 2\n",
      "Partida 29: 2\n",
      "Partida 30: 2\n",
      "Partida 31: 2\n",
      "Partida 32: 2\n",
      "Partida 33: 2\n",
      "Partida 34: 2\n",
      "Partida 35: 2\n",
      "Partida 36: 2\n",
      "Partida 37: 2\n",
      "Partida 38: 2\n",
      "Partida 39: 2\n",
      "Partida 40: 2\n",
      "Partida 41: 2\n",
      "Partida 42: 2\n",
      "Partida 43: 2\n",
      "Partida 44: 2\n",
      "Partida 45: 2\n",
      "Partida 46: 2\n",
      "Partida 47: 2\n",
      "Partida 48: 2\n",
      "Partida 49: 2\n",
      "Partida 50: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Esto es IA vs IA (NO EJECUTAR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# agente_td = AgenteTD(jugador=Conecta4.JUGADOR)  # Entrenado antes\n",
    "agente_minimax = AgenteIA(profundidad=4, poda_alpha_beta=False)\n",
    "agente_minimax_ab = AgenteIA(profundidad=4, poda_alpha_beta=True)\n",
    "\n",
    "# TD vs Minimax\n",
    "# res_td_vs_min = simular_enfrentamientos(agente, agente_minimax)\n",
    "\n",
    "# TD vs Minimax con poda\n",
    "res_td_vs_ab = simular_enfrentamientos(agente_td, agente_minimax_ab)\n",
    "\n",
    "# Minimax vs Minimax con poda\n",
    "# res_min_vs_ab = simular_enfrentamientos(agente_minimax, agente_minimax_ab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = ['Agente TD', 'Minimax', 'Minimax AB']\n",
    "victorias = [\n",
    "    res_td_vs_min[1],\n",
    "    res_td_vs_min[2],\n",
    "    0\n",
    "]\n",
    "\n",
    "victorias[0] += res_td_vs_ab[1]\n",
    "victorias[2] += res_td_vs_ab[2]\n",
    "victorias[1] += res_min_vs_ab[1]\n",
    "victorias[2] += res_min_vs_ab[2]\n",
    "\n",
    "plt.bar(labels, victorias)\n",
    "plt.title(\"Victorias de los Agentes en 150 partidas\")\n",
    "plt.ylabel(\"Cantidad de Victorias\")\n",
    "plt.savefig(\"resultados_agentes.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
